{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation\n",
    "\n",
    "In diesem Notebook m√∂chten wir uns mit der K√∂nigsdisziplin des Natural Language Processings besch√§ftigen, der maschinellen √úbersetzung.\n",
    "Vermutlich gibt es keine NLP-Anwendung, die einerseits vielen bekannt ist, anderseits durch Deep Learning und neuronalen Netzen einen solchen Aufschwung bekommen hat.\n",
    "Vor dem Siegeszug der neuronalen Netzen, wurde maschinelle √úbersetzung deshalb als schwierig angesehen, weil sie alle Teilaspekte von Sprache beinhaltet. Neben grammatikalischer Korrektheit, sollen maschinell √ºbersetzte Texte den Sinn des Originaltextes wiedergeben und dar√ºberhinaus auch den Subtext, wie Ironie, Witz, erfassen.\n",
    "Ob letzteres einfach gelingt, sei hier mal dahingestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten\n",
    "Im Vergleich zu den bisherigen Notebooks sind die Daten f√ºr machine translation etwas dr√∂ge. Daher m√∂chten wir uns hier nicht allzu lange aufhalten. Speichert zun√§chst folgende Datei ab und entpackt sie: http://www.manythings.org/anki/fra-eng.zip\n",
    "\n",
    "Danach wollen wir die Daten einlesen. Das Format ist einfach, pro Zeile steht ein Satzpaar getrennt von einem Tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fra.txt', 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [tuple(l.split(\"\\t\")) for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_cleaned = [(e, f.strip()) for e, f in pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit unser Modell wei√ü wo Anfang und Ende der franz√∂sischen S√§tze sind, m√ºssen wir noch ein spezielles Start- (üè≥Ô∏è) und Endsymbol (üè¥) einf√ºgen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_cleaned_with_marker = [(e, \"üè≥Ô∏è\" + f + \"üè¥\") for e, f in pairs_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichert euch nun alle Character, die in den englischen S√§tzen und in den franz√∂sischen S√§tzen in jeweils einem Set ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'üè≥Ô∏èVa !üè¥')\n"
     ]
    }
   ],
   "source": [
    "print(pairs_cleaned_with_marker[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/', '\"', '%', ';', '&', '‚Äô', 'N', '+', '1', 's', '√∫', 'e', 'D', \"'\", '7', 'f', '!', 'o', 'w', '4', ' ', '5', '$', 'W', '-', '√∂', 'n', 'q', '√ß', 'H', '‚Äò', '?', 'S', 'c', 'm', 'G', 'T', '‚Ç¨', '2', 'Q', 'r', 'J', 'b', 'Y', 't', 'L', 'i', 'k', '9', '‚Äî', 'x', '0', 'Z', ':', 'X', 'z', 'E', 'V', 'p', 'M', 'A', 'I', 'C', '3', '‚ÇÇ', '–∞', '¬∫', 'a', 'y', 'B', 'R', 'u', 'P', '.', 'F', '√©', 'd', '6', 'v', 'g', 'O', '\\xad', '8', 'j', 'U', ',', 'l', 'K', '\\xa0', 'h', '‚Äì']\n",
      "['/', '\"', '%', 'Ô∏è', ';', '‚ÄΩ', '¬´', '√ä', '‚Äô', '&', 'N', '√π', ')', '+', '1', 's', 'üè¥', '√á', 'e', 'D', '¬ª', \"'\", '7', 'f', '!', 'o', '\\u200b', 'w', '4', ' ', '5', '$', 'W', '-', '‚Ä¶', '√∂', 'n', 'q', '√ß', 'H', '‚Äò', '?', 'S', 'c', 'm', 'G', 'T', '√Æ', '√¥', 'Q', '2', 'r', 'J', 'b', '√â', 'Y', 't', 'L', 'i', 'k', '9', '√´', 'x', '0', 'Z', '–°', ':', '\\u2009', 'X', 'z', '√Ø', '√¢', '\\u202f', 'E', 'V', 'p', 'M', '(', 'A', 'I', 'C', '3', '‚ÇÇ', 'a', 'y', '√Ä', '≈ì', 'B', 'R', 'u', 'P', '.', 'F', '√ª', '√î', '√©', 'd', '6', 'v', '√†', 'g', 'O', '8', '√®', 'j', 'U', ',', '√™', 'l', 'K', '\\xa0', 'h', '√°', '‚Äì', '√Ç', 'üè≥']\n"
     ]
    }
   ],
   "source": [
    "english_characters = set()\n",
    "french_characters = set()\n",
    "\n",
    "for str_en, str_fr in pairs_cleaned_with_marker:\n",
    "    [english_characters.add(char) for char in str_en]\n",
    "    [french_characters.add(char) for char in str_fr]\n",
    "\n",
    "english_characters = list(english_characters)\n",
    "french_characters = list(french_characters)\n",
    "\n",
    "print(english_characters)\n",
    "print(french_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um unser Modell sp√§ter mit passenden Eingabedaten f√ºttern zu k√∂nnen, m√ºssen wir nun noch die jeweiligen Vektorl√§ngen f√ºr den encoder und den decoder bestimmen. Speichert die L√§ngen in den Variablen  `encoder_vector_len` und `decoder_vector_len` ab. Hinweis: uns Modell bekommt Vektoren, die die Characters pro Wort abbilden als Eingabe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "encoder_vector_len = len(english_characters)#TODO\n",
    "decoder_vector_len = len(french_characters)#TODO\n",
    "print(encoder_vector_len)\n",
    "print(decoder_vector_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Eingabevektoren bestimmen zu k√∂nnen, m√ºssen wir uns noch Dictionaries bauen, die jeweils die Character auf die Komponenten des Vektors abbilden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 50, '0': 51, '/': 0, '\"': 1, '%': 2, 'r': 40, 'Z': 52, ';': 3, ':': 53, 'X': 54, '&': 4, 'N': 6, 'E': 56, '+': 7, 's': 9, '1': 8, 'V': 57, 'p': 58, 'W': 23, 'M': 59, '√∫': 10, 'I': 61, '3': 63, '‚ÇÇ': 64, 'k': 47, 'e': 11, '–∞': 65, 'D': 12, \"'\": 13, '7': 14, '2': 38, 'f': 15, 'g': 79, '!': 16, 'o': 17, 'l': 86, 'w': 18, 'B': 69, 'q': 27, '4': 19, 'R': 70, ' ': 20, 'u': 71, 'P': 72, '.': 73, '-': 24, 'F': 74, '√∂': 25, '√©': 75, 'd': 76, '‚Äô': 5, 'n': 26, 'C': 62, '6': 77, '√ß': 28, 'v': 78, 'H': 29, '‚Äò': 30, '?': 31, 'S': 32, 'm': 34, 'O': 80, '5': 21, 'G': 35, '\\xad': 81, '8': 82, 'z': 55, 'T': 36, 'j': 83, '‚Ç¨': 37, 'U': 84, ',': 85, 'Q': 39, 'b': 42, 'J': 41, 'a': 67, '$': 22, 'A': 60, 'K': 87, 'Y': 43, '¬∫': 66, 't': 44, 'L': 45, 'i': 46, '‚Äî': 49, '9': 48, '\\xa0': 88, 'h': 89, 'c': 33, 'y': 68, '‚Äì': 90}\n",
      "{'x': 62, '0': 63, '/': 0, '\"': 1, '%': 2, 'b': 53, 'Z': 64, '–°': 65, ';': 4, ':': 66, '‚ÄΩ': 5, '¬´': 6, '√ä': 7, '\\u2009': 67, 'F': 92, 'X': 68, '‚Äô': 8, ')': 12, '√π': 11, '√Ø': 70, '√¢': 71, '\\u202f': 72, 'E': 73, '+': 13, 's': 15, '1': 14, 'V': 74, 'p': 75, '≈ì': 86, 'M': 76, '(': 77, '&': 9, 'A': 78, 'I': 79, 'üè¥': 16, '‚ÇÇ': 82, '√á': 17, 'e': 18, '.': 91, '¬ª': 20, \"'\": 21, '7': 22, '2': 50, '√Ä': 85, 'f': 23, 'g': 100, '!': 24, 'o': 25, 'Ô∏è': 3, '√®': 103, 'z': 69, '\\u200b': 26, 'w': 27, '√ª': 93, 'B': 87, 'q': 37, '4': 28, 'R': 88, '5': 30, 'u': 89, 'P': 90, '$': 31, '-': 33, '‚Ä¶': 34, '√î': 94, '√∂': 35, '√©': 95, 'd': 96, 'n': 36, 'C': 80, '6': 97, '√ß': 38, 'v': 98, '3': 81, '√†': 99, 'H': 39, '‚Äò': 40, '?': 41, 'S': 42, 'm': 44, 'O': 101, ' ': 29, 'G': 45, '8': 102, 'N': 10, 'T': 46, '√Æ': 47, 'j': 104, 'U': 105, ',': 106, 'Q': 49, 'r': 51, 'J': 52, '√â': 54, 'a': 83, '√™': 107, 'W': 32, 'l': 108, 'K': 109, 'Y': 55, 'D': 19, 't': 56, '√Ç': 114, 'L': 57, 'i': 58, 'k': 59, '9': 60, '\\xa0': 110, 'h': 111, '√°': 112, 'c': 43, 'y': 84, '‚Äì': 113, '√¥': 48, 'üè≥': 115, '√´': 61}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "english_token_index = {char:key for key, char in enumerate(english_characters)}\n",
    "french_token_index = {char:key for key, char in enumerate(french_characters)}\n",
    "\n",
    "print(english_token_index) #token2idx\n",
    "print(french_token_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben nun alle Dimensionen bestimmt, um die Trainingsdaten zu definieren. Daf√ºr bauen wir uns drei Matrizen (eine als Eingabe f√ºr den Encoder, eine als Eingabe f√ºr den Decoder und eine als Ausgabe des Decoders) zusammen. Die Dimensionen daf√ºr sind jeweils: Anzahl S√§tze, Anzahl W√∂rter des l√§ngsten Satzes, Anzahl Characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_encoder_sentence_len = max((len(e) for e, _ in pairs_cleaned_with_marker))\n",
    "max_decoder_sentence_len = max((len(f) for _, f in pairs_cleaned_with_marker))\n",
    "\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(pairs_cleaned_with_marker), max_encoder_sentence_len, encoder_vector_len),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(pairs_cleaned_with_marker), max_decoder_sentence_len, decoder_vector_len),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(pairs_cleaned_with_marker), max_decoder_sentence_len, decoder_vector_len),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bef√ºllt nun die Matrizen. Die `encoder_input_data` und die `decoder_input_data`-Matrix soll f√ºr jeden Satz aus den Trainingsdaten, einen Eintrag mit einem Token-Vektor pro Wort enthalten.\n",
    "Die `decoder_target_data`-Matrix soll den gleichen Inhalt wie die `decoder_input_data`-Matrix haben, allerdings um eine Position verschoben. (Schaut euch nochmal die Folien an, wir brauchen Platz f√ºr das Startsymbol) \n",
    "\n",
    "Die `encoder_input_data`-Matrix wird mit den englischen Daten bef√ºllt, die beiden anderen mit den franz√∂sischen Daten.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (e, f) in enumerate(pairs_cleaned_with_marker):\n",
    "    for t, char in enumerate(e):\n",
    "        encoder_input_data[i, t, english_token_index[char]] = 1.\n",
    "    for t, char in enumerate(f):\n",
    "        decoder_input_data[i, t, french_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t-1, french_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence Modelle\n",
    "\n",
    "Im Gegensatz zu den bisherigen Notebooks, wollen wir hier mit dem Modell beginnen. Einerseits, weil die Daten eher dr√∂ge  (trocken) sind (englisch-fr√§nzosische Satzpaare), andererseits, weil das Modell selbst komplizierter ist und wir hier die Functional-API von Keras benutzen m√ºssen.\n",
    "\n",
    "Im Gegensatz zu der Sequence-API, definiert man hier ein Modell in dem, den jeweiligen Layer mit dem Vorg√§nger Layer aufruft:\n",
    "```\n",
    "\n",
    "a = Input(...)\n",
    "b = Dense(...)\n",
    "\n",
    "t = b(a)\n",
    "\n",
    "\n",
    "```\n",
    "Als R√ºckgabe bekommt man einen Tensor.\n",
    "\n",
    "\n",
    "Ganz generell gesprochen, m√∂chten wir hier ein Seq2Seq-Modell aufbauen, das auf LSTMs basiert. Wie auf den Folien beschrieben wurde, teilt sich so ein Modell in einen Encoder und Decoder auf. \n",
    "Wir beginnen die Implementierung mit dem Encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Auch das komplexeste Modell f√§ngt mit einer Eingabe ein.\n",
    "\n",
    "Definiert einen Input-Layer, der als Eingabedimension, S√§tze beliebiger L√§nge als Vektoren nimmt. Eine Komponente im Vektor repr√§sentiert einen Character. Unbekannte Teile der Eingabedimensionen kann man mit `None` definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "encoder_input = Input(shape=(None, encoder_vector_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als n√§chstes definiert einen LSTM-Layer, der 256 hidden units hat. Die Ausgabe des Layers soll sp√§ter an den Decoder 'verf√ºttert' werden. Seht in der Dokumentation nach wie man den Thought-Vektor zur√ºckbekommen kann (https://keras.io/layers/recurrent/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "lstm_hu = 256\n",
    "encoder = LSTM(units=lstm_hu, return_state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbindet nun den `encoder_input`-Layer mit dem `encoder`-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "_, state_h, state_c = encoder(encoder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Encoder w√§re damit fertig implementiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Auch der Decoder f√§ngt mit einem Input-Layer an. Definiert einen Input-Layer analog zum Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None, decoder_vector_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie auch im Encoder, ist das Herzst√ºck ein LSTM-Layer. Definiert einen LSTM-Layer mit 256 hidden units. Seht in der Dokumentation nach, wie man neben den Thought-Vektoren, auch die komplette Ausgabe-Sequence zur√ºckbekommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_lstm = LSTM(256, return_state=True, return_sequences=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbindet nun den `decoder_input`-Layer mit dem `decoder_lstm`-Layer. An welcher der drei Ausgaben sind wir hier interessiert? Au√üerdem m√ºssen wir hier, den Thought-Vektor des Encoders mit √ºbergeben, dies geschieht √ºber den `initial_state`-Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences ,_,_ = decoder_lstm(decoder_input, initial_state=[state_h,state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Abschluss unseres Modells bildet ein Dense-Layer, der gleich viele `hidden_units` wie der Input-Layer des Decoders hat. Als Aktivierungsfunktion nehmen wir die Softmax-Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "decoder_layer = Dense(decoder_vector_len, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbindet diesen Layer mit den bisherigen Decoder sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder_layer(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben nun alle Elemente f√ºr ein Sequence-To-Sequence-Modell zusammen. Im letzten Schritt bauen wir das ganze Modell zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "\n",
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder])\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fertig! Nun kann das Modell trainiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 133704 samples, validate on 33426 samples\n",
      "Epoch 1/3\n",
      "133704/133704 [==============================] - 419s 3ms/step - loss: 0.2095 - val_loss: 0.3263\n",
      "Epoch 2/3\n",
      " 17152/133704 [==>...........................] - ETA: 5:18 - loss: 0.1622"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=256,\n",
    "          epochs=3,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferenz\n",
    "\n",
    "Um mit dem trainierten Modell Texte √ºbersetzen zu k√∂nnen, m√ºssen wir nun noch unser bisheriges Modell etwas umbauen.\n",
    "Im ersten Schritt definieren wir uns ein `Model` das einen Eingabetext encodiert. Als Eingabe hat dieses `Model` den bisherigen `encoder_input`-Layer und die Thought-Vektoren als Output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_input, [state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etwas komplizierter ist das Decoder-Modell. Als Input nimmt es zum einen die Thought-Vektoren des Encoders, zum anderen den `decoder_input`-Layer von oben.\n",
    "Definiert zun√§chst zwei Input-Layer f√ºr die Thought-Vektoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(lstm_hu,))\n",
    "decoder_state_input_c = Input(shape=(lstm_hu,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setzt diese beiden Input-Layer als `initial_state` und den `decoder_input`-Layer als Input in das `decoder_lstm` von oben ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_sequences, state_h, state_c = decoder_lstm(decoder_input, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setzt nun den `decoder_outputs`-Tensor in den `decoder_layer` von oben ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = decoder_layer(decoder_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir ein `Model` zusammenbauen, das uns als Decoder dient. Input ist hier der `decoder_input`-Layer, zusammen mit den `decoder_state_input_h` und dem `decoder_state_input_c`-Tensor. Als Output dient uns der `decoder_outputs`-Tensor und der `state_h` mit dem `state_c`-Tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = Model(inputs=[decoder_input, decoder_state_input_h, decoder_state_input_c], outputs=[decoder_outputs, state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein kleiner Zwischenschritt ist noch n√∂tig, bevor wir mit dem Modell Texte √ºbersetzen k√∂nnen. Damit wir aus unseren Vektoren wieder Texte bekommen, m√ºssen wir noch das Dictionary `reverse_french_token_index` bef√ºllen, das die \"Umkehrung\" von `french_token_index` ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_french_token_index = {idx:token for token, idx in french_token_index.items()} #idx2token\n",
    "reverse_english_token_index = {idx:token for token, idx in english_token_index.items()} #idx2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reverse_french_token_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun k√∂nnen wir eine Funktion `translate` implementieren, die als Eingabe einen englischen Text als Vektoren nimmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#first start: predict_char(french_token_index[\"üè≥\"], thought[0], thought[1]) -> french_char_idx, h, c\n",
    "#every iter: predict_char(french_char_idx, h, c) -> french_char_idx, h, c\n",
    "\n",
    "\n",
    "def predict_char(prev_char_idx, state_h, state_c):\n",
    "    french_char_onehot = np.zeros((1, 1, decoder_vector_len))\n",
    "    french_char_onehot[0, 0, prev_char_idx] = 1\n",
    "    predicted_chars_probs, h, c = decoder_model.predict([french_char_onehot, state_h, state_c])\n",
    "    predicted_french_char_idx = np.argmax(predicted_chars_probs)\n",
    "    predicted_french_char = reverse_french_token_index[predicted_french_char_idx]\n",
    "    return predicted_french_char_idx, predicted_french_char, h, c\n",
    "\n",
    "def translate(english_text):\n",
    "    # Wandelt mit dem encoder_model, `english_text` in einen Thought-Vektor um\n",
    "    thought = encoder_model.predict(english_text)\n",
    "    \n",
    "    translation = \"\"\n",
    "    \n",
    "    predicted_french_char_idx, h, c = french_token_index[\"üè≥\"], thought[0], thought[1]\n",
    "    \n",
    "    while predicted_french_char_idx != french_token_index[\"üè¥\"]:\n",
    "        predicted_french_char_idx, predicted_french_char, h, c = predict_char(predicted_french_char_idx, h, c)\n",
    "        #print(h[0][0])\n",
    "        translation += predicted_french_char\n",
    "    \n",
    "    return translation[:-1]\n",
    "\n",
    "    \"\"\"\n",
    "    ----------\n",
    "    # Als Input f√ºr den Decoder, wird zum einen der thought-Vektor ben√∂tigt, \n",
    "    # zum anderen die Vektor-Repr√§sentation des Startsymbols\n",
    "    french_text = np.zeros((1, 1, decoder_vector_len)) #TODO\n",
    "    \n",
    "    # Belegt die Komponente des Start-Symbols mit 1\n",
    "    french_text[0, 0, french_token_index[\"üè≥\"]] = 1.  #TODO\n",
    "    \n",
    "    # und √ºbergebt beides an den Decoder\n",
    "    chars, h, c = decoder_model.predict([french_text] + thought)\n",
    "    print(chars)\n",
    "    # chars beinhaltet eine 1 x 1 x decoder_vector_len Matrix, die f√ºr jeden franz√∂sische Character\n",
    "    # eine Art \"Wahrscheinlichkeit\" angibt.\n",
    "    # Findet die Komponente, mit der gr√∂√üten Wahrscheinlichkeit und wandelt sie in ein Character um\n",
    "    french_char_idx = np.argmax(chars) #TODO\n",
    "    french_char = reverse_french_token_index[french_char_idx]\n",
    "    print(french_char_idx, french_char)\n",
    "    \n",
    "    -------------------\n",
    "    # die n√§chste Prediction w√§re dann\n",
    "    french_text = np.zeros((1, 1, decoder_vector_len))\n",
    "    french_text[0, 0, french_char_idx] = 1.\n",
    "    chars, h, c = decoder_model.predict([french_text, h, c])\n",
    "    french_char_idx = np.argmax(chars) #TODO\n",
    "    french_char = reverse_french_token_index[french_char_idx]\n",
    "    print(french_char_idx, french_char)\n",
    "    -----------------\n",
    "    \"\"\"\n",
    "    # TODO: baut eine Schleife mit geeigneter Abbruchbedingung, die die Schritte von oben umfasst \n",
    "    # und dabei aus french_char einen √ºbersetzen Text zusammenbaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = np.array([encoder_input_data[10]])\n",
    "print(english_text.shape, len(english_text[0]), encoder_vector_len)\n",
    "\n",
    "french_text = translate(english_text)\n",
    "\n",
    "print(\"-- English --\")\n",
    "#print(pairs_cleaned_with_marker[10])\n",
    "print(\"\".join([reverse_english_token_index[np.argmax(char)] for char in english_text[0]]))\n",
    "print(\"-- French --\")\n",
    "print(french_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pairs_cleaned_with_marker[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: save model to disc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
